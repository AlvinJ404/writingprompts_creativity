{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creativity Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from scipy.spatial.distance import cosine\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "path_to_dataset_file = 'dataset/gpt-4/writing_prompts_train_subset.csv'\n",
    "\n",
    "df = pd.read_csv(path_to_dataset_file)\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['prompt'] = df['prompt'].astype(str)\n",
    "df['completion'] = df['completion'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "novelty (cosine similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cosine_similarity(text1, text2):\n",
    "    embeddings1 = model.encode(text1, convert_to_tensor=True).cpu()\n",
    "    embeddings2 = model.encode(text2, convert_to_tensor=True).cpu()\n",
    "    return 1 - cosine(embeddings1, embeddings2)\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "df['cosine'] = df.apply(lambda row: compute_cosine_similarity(row['prompt'], row['completion']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "surprise (perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained('gpt2').to(device)\n",
    "\n",
    "def compute_perplexity(text):\n",
    "    input_ids = tokenizer.encode(text, return_tensors='pt').to(device)\n",
    "    with torch.no_grad():\n",
    "        output = model(input_ids, labels=input_ids)\n",
    "        loss = output[0]\n",
    "        perplexity = torch.exp(loss)\n",
    "    return perplexity.item()\n",
    "\n",
    "df['ppl'] = df['completion'].apply(compute_perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "originality and flexibility (n-gram overlap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def compute_n_gram_overlap(text1, text2, n=3):\n",
    "    tokens1 = text1.split()\n",
    "    tokens2 = text2.split()\n",
    "    \n",
    "    if len(tokens1) < n or len(tokens2) < n:\n",
    "        return 0.0  \n",
    "    \n",
    "    counter1 = Counter(zip(*[tokens1[i:] for i in range(n)]))\n",
    "    counter2 = Counter(zip(*[tokens2[i:] for i in range(n)]))\n",
    "    \n",
    "    intersection = sum((counter1 & counter2).values())\n",
    "    union = sum((counter1 | counter2).values())\n",
    "    \n",
    "    if union == 0:\n",
    "        return 0.0\n",
    "        \n",
    "    return intersection / union\n",
    "\n",
    "df['n-gram_overlap'] = df.apply(lambda row: compute_n_gram_overlap(row['prompt'], row['completion']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fluency (n-gram transition probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def compute_n_gram_transition_probs(text, n=3):\n",
    "    tokens = text.split()\n",
    "    if len(tokens) < n:\n",
    "        return {}\n",
    "    \n",
    "    transitions = [tuple(tokens[i:i+n]) for i in range(len(tokens)-n+1)]\n",
    "    transition_counts = defaultdict(int)\n",
    "    \n",
    "    for transition in transitions:\n",
    "        transition_counts[transition] += 1\n",
    "    \n",
    "    total_transitions = len(transitions)\n",
    "    transition_probs = {transition: count / total_transitions for transition, count in transition_counts.items()}\n",
    "    return transition_probs\n",
    "\n",
    "def average_transition_probability(text, n=3):\n",
    "    probs = compute_n_gram_transition_probs(text, n)\n",
    "    return sum(probs.values()) / len(probs) if probs else 0\n",
    "\n",
    "df['n-gram_transition'] = df['completion'].apply(average_transition_probability)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "elaboration (Self-BLEU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from collections import Counter\n",
    "from typing import List, Tuple, Dict\n",
    "import numpy as np\n",
    "from nltk.util import ngrams\n",
    "\n",
    "def compute_self_bleu(text: List[str], ngram_weights: List[float] = [0.25, 0.25, 0.25, 0.25]) -> float:\n",
    "    \"\"\"\n",
    "    Compute self-BLEU score for a list of sentences using GPU acceleration.\n",
    "    \n",
    "    Args:\n",
    "        text (List[str]): List of sentences to compute self-BLEU for\n",
    "        ngram_weights (List[float]): Weights for different n-gram orders (default: equal weights for 1-4 grams)\n",
    "    \n",
    "    Returns:\n",
    "        float: Self-BLEU score\n",
    "    \"\"\"\n",
    "    if not torch.cuda.is_available():\n",
    "        raise RuntimeError(\"GPU not available. Please ensure CUDA is installed and a GPU is accessible.\")\n",
    "    \n",
    "    device = torch.cuda.current_device()\n",
    "    \n",
    "    def get_ngrams(sentence: str, n: int) -> Counter:\n",
    "        \"\"\"Generate n-grams from a sentence.\"\"\"\n",
    "        tokens = sentence.lower().split()\n",
    "        return Counter(tuple(gram) for gram in ngrams(tokens, n))\n",
    "    \n",
    "    def prepare_ngram_matches(candidates: List[str], n: int) -> Tuple[torch.Tensor, Dict]:\n",
    "        \"\"\"\n",
    "        Prepare n-gram match tensors for GPU computation.\n",
    "        Returns tensor of shape (num_sentences, unique_ngrams) and ngram mapping.\n",
    "        \"\"\"\n",
    "        # Collect all unique n-grams\n",
    "        all_ngrams = set()\n",
    "        for sent in candidates:\n",
    "            all_ngrams.update(get_ngrams(sent, n).keys())\n",
    "        \n",
    "        # Create mapping from n-gram to index\n",
    "        ngram_to_idx = {gram: idx for idx, gram in enumerate(all_ngrams)}\n",
    "        \n",
    "        # Create tensor of n-gram counts\n",
    "        ngram_counts = torch.zeros((len(candidates), len(ngram_to_idx)), dtype=torch.float32, device=device)\n",
    "        \n",
    "        for i, sent in enumerate(candidates):\n",
    "            sent_ngrams = get_ngrams(sent, n)\n",
    "            for gram, count in sent_ngrams.items():\n",
    "                if gram in ngram_to_idx:\n",
    "                    ngram_counts[i, ngram_to_idx[gram]] = count\n",
    "        \n",
    "        return ngram_counts, ngram_to_idx\n",
    "\n",
    "    def calculate_bleu_stats_gpu(ref_ngrams: torch.Tensor, hyp_ngrams: torch.Tensor) -> Tuple[float, float]:\n",
    "        \"\"\"Calculate BLEU statistics using GPU tensor operations.\"\"\"\n",
    "        # Calculate matches and totals\n",
    "        matches = torch.minimum(ref_ngrams, hyp_ngrams).sum().item()\n",
    "        total = hyp_ngrams.sum().item()\n",
    "        \n",
    "        return matches, total\n",
    "\n",
    "    scores = []\n",
    "    \n",
    "    # Calculate self-BLEU for each n-gram order\n",
    "    for n, weight in enumerate(ngram_weights, start=1):\n",
    "        if weight == 0:\n",
    "            continue\n",
    "            \n",
    "        total_precision = 0\n",
    "        ngram_counts, ngram_map = prepare_ngram_matches(text, n)\n",
    "        \n",
    "        # For each sentence, compare with all other sentences\n",
    "        for i in range(len(text)):\n",
    "            hypothesis = ngram_counts[i:i+1]  # Keep dimension for broadcasting\n",
    "            references = torch.cat([ngram_counts[:i], ngram_counts[i+1:]], dim=0)\n",
    "            \n",
    "            # Calculate maximum matches with any reference\n",
    "            matches, total = calculate_bleu_stats_gpu(references, hypothesis)\n",
    "            \n",
    "            if total > 0:\n",
    "                precision = matches / total\n",
    "                total_precision += precision\n",
    "        \n",
    "        if len(text) > 1:\n",
    "            avg_precision = total_precision / (len(text))\n",
    "            scores.append((weight, avg_precision))\n",
    "    \n",
    "    # Calculate final weighted BLEU score\n",
    "    final_score = 0\n",
    "    sum_weights = sum(weight for weight, _ in scores)\n",
    "    \n",
    "    if sum_weights > 0:\n",
    "        final_score = sum(weight * score for weight, score in scores) / sum_weights\n",
    "        \n",
    "    # Clear GPU memory\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return final_score\n",
    "\n",
    "\n",
    "df['self-bleu'] = df['completion'].apply(compute_self_bleu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "elaboration (length by number of words and chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['len_word'] = df['completion'].str.split().str.len()\n",
    "df['len_char'] = df['completion'].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(path_to_dataset_file, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
